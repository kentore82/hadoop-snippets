{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARK documentation\n",
    "https://spark.apache.org/docs/latest/api/python/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-14T13:12:04.512312",
     "start_time": "2016-10-14T13:12:04.506530"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Spark bindings\n",
    "execfile(\"/etc/spark/conf/spark_1.6.0_binings.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set distribution mode, appname, and claim resources\n",
    "master='yarn-client' #\"yarn-client\" to run distributed mode in yarn, \"local\" to run local\n",
    "AppName=\"Spark Template\"\n",
    "num_executors=1\n",
    "exec_memory=1 #in GigaByte pr. executor. Tot mem = num_executors*exec_memory\n",
    "driver_memory=1 #in GigaByte.\n",
    "\n",
    "\n",
    "#############--==DO NOT EDIT==--###############\n",
    "from pyspark import SparkConf\n",
    "sconf=SparkConf()\n",
    "\n",
    "sconf.set('spark.master',master)\n",
    "sconf.set('spark.executor.instances',str(num_executors))#Number of executors\n",
    "#sconf.set('spark.shuffle.service.enabled',True)\n",
    "#sconf.set('spark.dynamicAllocation.enabled',True)\n",
    "sconf.set('spark.executor.memory',str(exec_memory)+'g')\n",
    "sconf.set('spark.driver.memory',str(driver_memory)+'g')\n",
    "#sconf.set('spark.executor.cores','2') # number of cores on same worker\n",
    "sconf.set('spark.app.name',AppName) #Application Name\n",
    "sconf.set('spark.app.id',AppName)\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(conf=sconf)\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-14T13:25:57.786709",
     "start_time": "2016-10-14T13:25:48.085314"
    },
    "collapsed": true
   },
   "source": [
    "## After running the two first cells. Go to the YARN application manager to verify that your job/ Spark Context is running:\n",
    "\n",
    "http://10.10.110.1:8088/cluster/scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-14T13:28:02.551883",
     "start_time": "2016-10-14T13:28:02.543406"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Spark DataFrame API's\n",
    "from pyspark import HiveContext\n",
    "sqlContext = HiveContext(sc)\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: when you are finished with your SPARK Notebook, remember to run \"sc.stop()\" to release recourses.\n",
    "\n",
    "## Spark example using LinkedIn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-14T13:29:55.261758",
     "start_time": "2016-10-14T13:29:55.035474"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load to Spark\n",
    "raw_data_spark=sc.textFile(\"/tmp/hadoop_nasjon_messy.txt\")\n",
    "raw_data_spark.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-14T13:30:00.380621",
     "start_time": "2016-10-14T13:30:00.138691"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop header\n",
    "raw_data_spark1 = raw_data_spark.filter(lambda row: row[0][0] != '#')\\\n",
    ".map(lambda row: row.split(','))\n",
    "raw_data_spark1.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-14T13:30:11.131357",
     "start_time": "2016-10-14T13:30:11.016130"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parse Norway 273 as (273), Norway \n",
    "raw_data_spark2 = raw_data_spark1\\\n",
    ".map(lambda row: [row[0].split(' ')[1],row[0].split(' ')[0]]+row[1:])\n",
    "\n",
    "raw_data_spark2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-14T13:43:16.474343",
     "start_time": "2016-10-14T13:43:16.384050"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove unwanted chars and set dtypes\n",
    "raw_data_spark3=raw_data_spark2\\\n",
    ".map(lambda row: [i.strip('()') for i in row])\\\n",
    ".map(lambda row: [int(i) if i.isdigit() else i for i in row])\n",
    "\n",
    "raw_data_spark3.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-14T13:48:07.974128",
     "start_time": "2016-10-14T13:48:07.648153"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data is parsed. Create schema and dataframe\n",
    "schema = StructType([\\\n",
    "    StructField(\"num_country\", IntegerType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"num_top1\", IntegerType(), True),\n",
    "    StructField(\"top1_company\", StringType(), True),\n",
    "    StructField(\"num_top2\", IntegerType(), True),\n",
    "    StructField(\"top2_company\", StringType(), True),\n",
    "    StructField(\"num_top3\", IntegerType(), True),\n",
    "    StructField(\"top3_company\", StringType(), True),\n",
    "    StructField(\"num_top4\", IntegerType(), True),\n",
    "    StructField(\"top4_company\", StringType(), True),\n",
    "    StructField(\"num_top5\", IntegerType(), True),\n",
    "    StructField(\"top5_company\", StringType(), True)])\n",
    "    \n",
    "hadoop_users = sqlContext.createDataFrame(raw_data_spark3, schema)\n",
    "hadoop_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-14T13:54:16.727692",
     "start_time": "2016-10-14T13:54:16.475484"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You now have a Spark DataFrame, and can use the Spark SQL API if you register it as a table.\n",
    "sqlContext.registerDataFrameAsTable(hadoop_users, \"hadoop_users\")\n",
    "sqlContext.sql(\"SELECT * FROM hadoop_users WHERE country = 'Norway'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-14T13:55:37.792670",
     "start_time": "2016-10-14T13:55:36.902936"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  },
  "toc": {
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
