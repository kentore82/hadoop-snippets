{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-13T19:30:16.919934",
     "start_time": "2017-01-13T19:30:16.910686"
    },
    "collapsed": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Import Spark bindings\n",
    "execfile(\"/share/hadoop_custom/conf/spark/spark_1.6.0_binings.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-13T19:30:23.064749",
     "start_time": "2017-01-13T19:30:23.049601"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import ais\n",
    "import time\n",
    "import numpy as np\n",
    "import threading\n",
    "import Queue\n",
    "from datetime import datetime\n",
    "import happybase\n",
    "from Geohash import geohash\n",
    "\n",
    "#plotly\n",
    "import plotly\n",
    "import plotly.plotly as py  \n",
    "import plotly.tools as tls   \n",
    "import plotly.graph_objs as go\n",
    "import randomcolor\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-13T19:30:58.342588",
     "start_time": "2017-01-13T19:30:57.267161"
    },
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending the driver its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4641b64cdde3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#############--==DO NOT EDIT==--###############\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0msconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0msconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.master'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-5.8.3-1.cdh5.8.3.p0.2/lib/spark/python/pyspark/conf.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loadDefaults, _jvm, _jconf)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jvm\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloadDefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-5.8.3-1.cdh5.8.3.p0.2/lib/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-5.8.3-1.cdh5.8.3.p0.2/lib/spark/python/pyspark/java_gateway.pyc\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mcallback_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgateway_port\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java gateway process exited before sending the driver its port number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# In Windows, ensure the Java child processes do not linger after Python has exited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Java gateway process exited before sending the driver its port number"
     ]
    }
   ],
   "source": [
    "# Set distribution mode, appname, and claim resources\n",
    "master='yarn-client' #\"yarn\" to run distributed mode in yarn, \"local\" to run local\n",
    "#dmode='client' #spark2.0 only\n",
    "AppName=\"AIS - streaming Kystverket\"\n",
    "num_executors=2\n",
    "exec_memory=1 #in GigaByte pr. executor. Tot mem = num_executors*exec_memory\n",
    "driver_memory=1 #in GigaByte.\n",
    "\n",
    "\n",
    "\n",
    "#############--==DO NOT EDIT==--###############\n",
    "from pyspark import SparkConf\n",
    "sconf=SparkConf()\n",
    "\n",
    "sconf.set('spark.master',master)\n",
    "#sconf.set('spark.submit.deployMode',dmode) #spark2.0 only\n",
    "sconf.set('spark.executor.instances',str(num_executors))#Number of executors\n",
    "#sconf.set('spark.shuffle.service.enabled',True)\n",
    "#sconf.set('spark.dynamicAllocation.enabled',True)\n",
    "sconf.set('spark.executor.memory',str(exec_memory)+'g')\n",
    "sconf.set('spark.driver.memory',str(driver_memory)+'g')\n",
    "#sconf.set('spark.executor.cores','2') # number of cores on same worker\n",
    "sconf.set('spark.app.name',AppName) #Application Name\n",
    "sconf.set('spark.app.id',AppName)\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "sc = SparkContext(conf=sconf)\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-11T09:53:25.510794",
     "start_time": "2016-11-11T09:53:25.475156"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#STREAM DATA\n",
    "threads = []\n",
    "q = Queue.Queue()\n",
    "\n",
    "if 't_list' in locals() or 'ssc' in locals():\n",
    "    del t_list\n",
    "    del ssc\n",
    "batch_interval=1#Seconds\n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "\n",
    "#Kystverket's open streaming connection:\n",
    "streaming_host=\"153.44.253.27\"\n",
    "streaming_port=5631\n",
    "\n",
    "#Set region of interest\n",
    "#bbox=[lllat,lllon,urlat,urlon]\n",
    "bbox_oslo=[59.0, 10.224365,59.881444, 11.728791]# <- Oslofjorden ll \n",
    "bbox_full=[0,0,100,100]\n",
    "  \n",
    "\n",
    "threads.append(threading.Thread(target=spark_stream,\\\n",
    "                                args=(sc, ssc,streaming_host,streaming_port,q,bbox_full)))\n",
    "\n",
    "\n",
    "t_list=[t.start() for t in threads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-11T09:53:18.619704",
     "start_time": "2016-11-11T09:53:18.307191"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stop StreamingContext\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-13T19:27:04.681618",
     "start_time": "2017-01-13T19:27:04.639407"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stop SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-11T09:53:15.487878",
     "start_time": "2016-11-11T09:52:28.042393"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxpoints=20\n",
    "num_ship=25\n",
    "mmsi_dict=follow_mmsi(q,maxpoints,num_ship)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-11T09:53:33.976212",
     "start_time": "2016-11-11T09:53:32.385343"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_streaming_plot('AISstream',mmsi_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-11T10:06:22.758405",
     "start_time": "2016-11-11T10:00:56.977665"
    },
    "cell_style": "center",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    #init stream data to plot\n",
    "    #Init streaming\n",
    "    ship_info_dict,plotly_stream_dict=plotly_stream_init(mmsi_dict)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        ship_info_dict_new=acc_points(ship_info_dict.copy(),q.get(),maxpoints)\n",
    "\n",
    "        #Get updated keys\n",
    "        kupdate=[]\n",
    "        for key in mmsi_dict.keys():\n",
    "            #print ship_info_dict[key]\n",
    "            #print ship_info_dict_new[key]\n",
    "            #print \"-...-\"\n",
    "            if ship_info_dict_new[key]==ship_info_dict[key]:\n",
    "                pass\n",
    "            else:\n",
    "                ship_info_dict[key]=ship_info_dict_new[key]\n",
    "                lets_stream({key:ship_info_dict[key]},{key:plotly_stream_dict[key]},{key:mmsi_dict[key]}) #test_stream,s)#\n",
    "\n",
    "    #         try:\n",
    "    #             lets_stream(ship_info_dict,plotly_stream_dict,mmsi_dict) #test_stream,s)#\n",
    "\n",
    "    #         except Exception as e:\n",
    "    #             print(str(e))\n",
    "    #             plotly_stream_close(plotly_stream_dict)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    plotly_stream_close(plotly_stream_dict)\n",
    "    print('Aborting on Ctrl-c, goodbye!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-11T10:00:36.884010",
     "start_time": "2016-11-11T10:00:36.876528"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iframe = '<iframe width=\"1000\" height=\"800\" frameborder=\"0\" scrolling=\"no\" src=\"//plot.ly/~kentt/181.embed\"></iframe>'\n",
    "IPython.display.HTML(iframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-11T09:51:36.502018",
     "start_time": "2016-11-11T09:51:35.845575"
    },
    "collapsed": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "#INIT STREAMING PLOT\n",
    "def init_streaming_plot(figname,mmsi_dict):\n",
    "    # init mapboxplot, data and layout\n",
    "    mapbox_access_token = \"pk.eyJ1Ijoia2VudHQiLCJhIjoiY2l1dHIyMmsyMDAwZTJ5czlwNTY4c3E2ZCJ9.IcUEo9TXPyTiwMmrEiikvQ\"\n",
    "\n",
    "    data = []\n",
    "    for key in mmsi_dict.keys():\n",
    "        data_init=dict(\n",
    "            type='scattermapbox',\n",
    "            lon=[],\n",
    "            lat=[],\n",
    "            mode='markers',\n",
    "            marker={'size':10,'color':mmsi_dict[key]['color']},\n",
    "            stream=mmsi_dict[key]['stream_id'],\n",
    "            name=mmsi_dict[key][\"name\"])\n",
    "\n",
    "        data.append(data_init)\n",
    "\n",
    "    layout = go.Layout(\n",
    "        autosize=True,\n",
    "        hovermode='closest',\n",
    "        mapbox=dict(\n",
    "            accesstoken=mapbox_access_token,\n",
    "            bearing=0,\n",
    "            center=dict(\n",
    "                lon=10,\n",
    "                lat=59\n",
    "            ),\n",
    "            pitch=60,\n",
    "            zoom=7\n",
    "        ),\n",
    "        width  = '1000',\n",
    "        height = '800',\n",
    "    )\n",
    "\n",
    "    fig = dict(data=data, layout=layout)\n",
    "    py.iplot(fig, filename=figname)\n",
    "\n",
    "def plotly_stream_init(mmsi_dict):\n",
    "    ship_info_dict={}\n",
    "    plotly_stream_dict={}\n",
    "    \n",
    "    for key in mmsi_dict.keys():\n",
    "        #Init empty data dict\n",
    "        ship_info_dict[key]=[[],[],[],[],[],[],[],[],[],[],[]]\n",
    "        #Init streaming objects\n",
    "        plotly_stream_dict[key]=py.Stream(stream_id=mmsi_dict[key][\"token\"])\n",
    "        plotly_stream_dict[key].open()\n",
    "\n",
    "    return ship_info_dict,plotly_stream_dict\n",
    "\n",
    "def plotly_stream_close(plotly_stream_dict):\n",
    "    for key in plotly_stream_dict.keys():\n",
    "        plotly_stream_dict[key].close()\n",
    "\n",
    "\n",
    "#Get list of mmsi's to follow\n",
    "def follow_mmsi(q,mpoints,num_ship):\n",
    "    #Accumulate q.get() to get a good base to pick mmsi's\n",
    "    q_acc=[]\n",
    "    \n",
    "    stream_tokens = tls.get_credentials_file()['stream_ids']\n",
    "    mmsi_stream_token={}\n",
    "    \n",
    "    while len(mmsi_stream_token.keys())<num_ship:\n",
    "        mmsi_list=[]\n",
    "        q_acc.append(q.get())\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        for item in q_acc:\n",
    "            for sub_item in item:\n",
    "                mmsi_list.append(sub_item[0])\n",
    "\n",
    "        mmsi_list = list(sorted(set(mmsi_list)))\n",
    "\n",
    "        #Check HBase if ship name is found:\n",
    "        for i in range(0,len(mmsi_list)):\n",
    "            mmsi=mmsi_list[i]\n",
    "\n",
    "            info_dict=get_meta_from_mmsi(str(mmsi))\n",
    "            mmsi_list[i]=(mmsi,info_dict['P:name'])\n",
    "\n",
    "        if len(mmsi_list)>len(stream_tokens):\n",
    "            N=len(stream_tokens)\n",
    "        else:\n",
    "            N=len(mmsi_list)\n",
    "\n",
    "        for i in range(0,N):\n",
    "            token=stream_tokens[i]\n",
    "            mmsi=mmsi_list[i][0]\n",
    "            name=mmsi_list[i][1]\n",
    "\n",
    "            if name=='not_found':\n",
    "                continue\n",
    "            else:\n",
    "                mmsi_stream_token[mmsi]={\"token\":token,\"name\":name,\"mmsi\":mmsi,\"stream_id\":dict(token=token, maxpoints=mpoints),\"color\":randomcolor.RandomColor().generate()[0]}\n",
    "\n",
    "    return mmsi_stream_token\n",
    "\n",
    "def acc_points(ship_info_dict,q,maxpoints):\n",
    "    for row in q:\n",
    "        try:\n",
    "            ship_info=ship_info_dict[row[0]][:]\n",
    "            for i in range(0,len(ship_info)):\n",
    "                ship_info[i]=ship_info[i]+[row[i]]\n",
    "\n",
    "            ship_info_dict[row[0]]=ship_info\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    #Limit the number of accumulated points\n",
    "    for key in ship_info_dict.keys():\n",
    "        if len(ship_info_dict[key][0])>maxpoints:\n",
    "            for i in range(0,len(ship_info_dict[key])):\n",
    "                ship_info_dict[key][i]=ship_info_dict[key][i][-maxpoints:]\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "    return ship_info_dict\n",
    "\n",
    "def lets_stream(ship_info_dict,plotly_stream_dict,mmsi_dict):\n",
    "    \n",
    "    for key in mmsi_dict.keys():\n",
    "        if ship_info_dict[key][0]==[]:\n",
    "            continue\n",
    "        \n",
    "        ais=ship_info_dict[key][:]\n",
    "        \n",
    "        col=mmsi_dict[key][\"color\"]\n",
    "        \n",
    "        imo = ais[8]\n",
    "        name = ais[9]\n",
    "        stype = ais[10]\n",
    "        SOG = ais[5]\n",
    "        COG = ais[7]\n",
    "        dtime = ais[1]\n",
    "        x_lon = ais[2]\n",
    "        y_lat = ais[3] \n",
    "        \n",
    "        plotly_stream_dict[key].write(go.Scattermapbox(lon=x_lon,\n",
    "                        lat=y_lat,\n",
    "                        marker=go.Marker(color=[col for i in range(0,len(imo)-1)]+[\"black\"]),\n",
    "                        text=['IMO: '+str(imo[i])+'<br>'\\\n",
    "                              +'Name: '+str(name[i])+'<br>'\\\n",
    "                              +'Type: '+str(stype[i])+'<br>'\\\n",
    "                              +'Time: '+datetime.fromtimestamp(int(dtime[i])).strftime('%Y-%m-%d %H:%M:%S')+'<br>'\\\n",
    "                              +'COG: '+str(int(COG[i]))+'<br>'+'SOG: '+str(int(SOG[i])) for i in range(0,len(imo))]))\n",
    "    \n",
    "def spark_stream(sc, ssc,streaming_host,streaming_port,q,bbox):\n",
    "    #Connect to stream\n",
    "    nmea = ssc.socketTextStream(streaming_host, streaming_port)\n",
    "    \n",
    "    # Decode and filter bad messages\n",
    "    nmea_decoded = nmea.map(lambda x: try_decode(x,bbox))\n",
    "    nmea_decoded = nmea_decoded.filter(lambda x:x!=[])\n",
    "        \n",
    "    # Connect to HBase and add metadata about vessel\n",
    "    nmea_decoded = nmea_decoded.map(lambda x: x+[get_meta_from_mmsi(str(x[0]))[\"P:imo\"],\\\n",
    "                                                 get_meta_from_mmsi(str(x[0]))[\"P:name\"],\\\n",
    "                                                 get_meta_from_mmsi(str(x[0]))[\"P:type\"]])\n",
    "    \n",
    "    #since rdd is small collect and save to local FS\n",
    "    #nmea_decoded.map(lambda x: rdd_list_to_str(x)).foreachRDD(lambda rdd: save_to_local(rdd.collect()))\n",
    "    \n",
    "    # Collect and put in que\n",
    "    nmea_decoded.foreachRDD(lambda t, rdd: q.put(rdd.collect()))\n",
    "    \n",
    "    # Run!\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination(timeout=10)\n",
    "    \n",
    "def save_to_local(rdd_collected):\n",
    "    filen='ais_'+str(int(time.time()))\n",
    "    dirpath=\"/STAGING/DATASETS/AIS/dump08112016/\"\n",
    "    with open(dirpath+filen, 'w') as file_handler:\n",
    "        for item in rdd_collected:\n",
    "            file_handler.write(item+'\\n')\n",
    "    os.system(\"chmod 777 \"+dirpath+filen)\n",
    "    \n",
    "\n",
    "def rdd_list_to_str(rdd_list): \n",
    "    rdd_str=''\n",
    "    for el in rdd_list:\n",
    "        rdd_str=rdd_str+','+str(el)\n",
    "    \n",
    "    return rdd_str[1:]\n",
    "\n",
    "def get_meta_from_mmsi(mmsi):\n",
    "    #Create connection\n",
    "    connection = happybase.Connection('2.sherpa.client.sysedata.no')\n",
    "    connection.open()\n",
    "\n",
    "    table_name=\"mmsiShipInfo\"\n",
    "    table = connection.table(table_name)\n",
    "    info_dict=table.row(mmsi)\n",
    "    if info_dict=={}:\n",
    "        info_dict={'P:imo': 'not_found','P:mmsi': mmsi,'P:name': 'not_found','P:type': 'not_found'}\n",
    "    \n",
    "    connection.close()\n",
    "    return info_dict\n",
    "\n",
    "def try_decode(nmea,bbox):\n",
    "    #bbox=[lllat,lllon,urlat,urlon]\n",
    "    try:\n",
    "        x=decode_nmea_no_prefix(nmea)\n",
    "        lat=x['y']\n",
    "        lon=x['x']\n",
    "        \n",
    "        if lat > bbox[0] and lat < bbox[2] and lon > bbox[1] and lon < bbox[3]:\n",
    "            decoded_list=[int(x['mmsi']),x['unixtime'],float(x['x']),float(x['y']),x['geohash'],float(x['sog']),float(x['rot']),float(x['cog'])]\n",
    "        else:\n",
    "            decoded_list=[]\n",
    "            \n",
    "    except:\n",
    "        decoded_list=[]\n",
    "\n",
    "    return decoded_list\n",
    "\n",
    "def decode_nmea_no_prefix(nmea):\n",
    "    commasplit=nmea.split(',')\n",
    "    \n",
    "    nmea_talkerid=commasplit[1].split('\\\\')[-1]\n",
    "    fragment_no=commasplit[3]\n",
    "    seq_message_id=commasplit[4]\n",
    "    payload=commasplit[-2]\n",
    "    fill_bits=int(commasplit[-1][0])\n",
    "\n",
    "    #Decode ais payload\n",
    "    msg_type=[]\n",
    "    try:\n",
    "        aisdata=ais.decode(payload,fill_bits)\n",
    "        msg_type=int(aisdata['id'])\n",
    "    except:\n",
    "        try:\n",
    "            fill_bits=2\n",
    "            aisdata=ais.decode(payload,fill_bits)\n",
    "            msg_type=int(aisdata['id'])\n",
    "        except:\n",
    "            msg_type=30\n",
    "            aisdata={'id':msg_type}\n",
    "    if msg_type==20:\n",
    "        aisdata=unroll_msg20(aisdata)\n",
    "\n",
    "    if 'x' in aisdata and 'y' in aisdata: # and 'x'!=181 and 'y'!=91: # x- longitude , y- latitude\n",
    "        try:\n",
    "            aisdata[u'geohash'] = geohash.encode(aisdata['y'],aisdata['x'],13)\n",
    "        except:\n",
    "            aisdata[u'geohash'] = '0'\n",
    "\n",
    "\n",
    "    #Append NMEA Tag Blocks         \n",
    "    aisdata[u'unixtime'] = int(time.time()) # since no timestamp is included, set it to utc.now\n",
    "    aisdata[u'n_talkerid'] = nmea_talkerid\n",
    "    aisdata[u'n_fragmentno'] = fragment_no\n",
    "    aisdata[u'n_seqmsg'] = seq_message_id\n",
    "    aisdata[u'n_aispayload'] = payload\n",
    "    aisdata[u'n_fillbits'] = fill_bits\n",
    "   \n",
    "    return aisdata"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialisation Cell",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
